"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ ML-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è SmartSwipe
–í–∫–ª—é—á–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤: Content-Based, User-Based, Ensemble
"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import joblib
import os
from typing import List, Dict, Tuple, Optional
import uuid
from datetime import datetime

from ..models import User, Idea, Swipe, IdeaView
from ..crud.swipe import get_user_likes, get_user_swipe_history


class AdvancedRecommender:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    
    def __init__(self, model_dir: str = "backend/ml_models"):
        self.model_dir = model_dir
        os.makedirs(model_dir, exist_ok=True)
        
        # –ú–æ–¥–µ–ª–∏
        self.content_model = None
        self.user_model = None
        self.ensemble_model = None
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.domain_encoder = LabelEncoder()
        self.scaler = StandardScaler()
        
        # –ú–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–¥—Å—Ç–≤–∞
        self.content_similarity_matrix = None
        self.user_similarity_matrix = None
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.ideas_df = None
        self.users_df = None
        self.training_metrics = {}
        
    
    def _prepare_idea_features(self, ideas: List[Idea]) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–¥–µ–π"""
        
        data = []
        for idea in ideas:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º title, description, tags –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
            combined_text = f"{idea.title} {idea.description} {' '.join(idea.tags)}"
            
            data.append({
                'id': str(idea.id),
                'title': idea.title,
                'description': idea.description,
                'tags': idea.tags,
                'domain': idea.domain,
                'combined_text': combined_text,
                'text_length': len(combined_text),
                'tag_count': len(idea.tags),
                'created_at': idea.created_at
            })
        
        df = pd.DataFrame(data)
        
        if len(df) > 0:
            # –ö–æ–¥–∏—Ä—É–µ–º –¥–æ–º–µ–Ω—ã
            df['domain_encoded'] = self.domain_encoder.fit_transform(df['domain'])
            
            # TF-IDF –¥–ª—è —Ç–µ–∫—Å—Ç–∞
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(df['combined_text'])
            
            # –î–æ–±–∞–≤–ª—è–µ–º TF-IDF –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫–∏
            tfidf_df = pd.DataFrame(
                tfidf_matrix.toarray(),
                columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])]
            )
            
            df = pd.concat([df.reset_index(drop=True), tfidf_df], axis=1)
        
        return df
    
    
    def _prepare_user_features(self, db_session, users: List[User]) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        
        data = []
        for user in users:
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_likes = get_user_likes(db_session, user.id)
            user_history = get_user_swipe_history(db_session, user.id)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–º–µ–Ω–æ–≤
            liked_domains = [like.idea.domain for like in user_likes if like.idea]
            domain_stats = {}
            for domain in set(liked_domains):
                domain_stats[f'liked_{domain}'] = liked_domains.count(domain)
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_swipes = len(user_history)
            total_likes = len(user_likes)
            like_ratio = total_likes / total_swipes if total_swipes > 0 else 0
            
            # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–≥–æ–≤ –≤ –ª–∞–π–∫–Ω—É—Ç—ã—Ö –∏–¥–µ—è—Ö
            avg_tags_per_like = np.mean([len(like.idea.tags) for like in user_likes if like.idea]) if user_likes else 0
            
            # –î–æ–ª—è –ª–∞–π–∫–Ω—É—Ç—ã—Ö –¥–æ–º–µ–Ω–æ–≤
            user_domains = user.selected_domains or []
            liked_domains_ratio = len(set(liked_domains)) / len(user_domains) if user_domains else 0
            
            user_data = {
                'id': str(user.id),
                'total_swipes': total_swipes,
                'total_likes': total_likes,
                'like_ratio': like_ratio,
                'avg_tags_per_like': avg_tags_per_like,
                'liked_domains_ratio': liked_domains_ratio,
                'swipe_history_length': total_swipes,
                'selected_domains_count': len(user_domains),
                **domain_stats
            }
            
            data.append(user_data)
        
        return pd.DataFrame(data)
    
    
    def _prepare_training_data(self, db_session) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å–≤–∞–π–ø—ã
        swipes = db_session.query(Swipe).join(Idea).join(User).all()
        
        training_data = []
        labels = []
        
        for swipe in swipes:
            if not swipe.idea or not swipe.user:
                continue
                
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–¥–µ–∏
            idea = swipe.idea
            combined_text = f"{idea.title} {idea.description} {' '.join(idea.tags)}"
            
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user = swipe.user
            user_likes = get_user_likes(db_session, user.id)
            user_history = get_user_swipe_history(db_session, user.id)
            
            # –í–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features = [
                # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–¥–µ–∏
                len(combined_text),  # –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
                len(idea.tags),      # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤
                self.domain_encoder.fit_transform([idea.domain])[0] if hasattr(self.domain_encoder, 'classes_') else 0,
                
                # –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                len(user_history),   # –∏—Å—Ç–æ—Ä–∏—è —Å–≤–∞–π–ø–æ–≤
                len(user_likes),     # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∞–π–∫–æ–≤
                len(user_likes) / len(user_history) if user_history else 0,  # —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ª–∞–π–∫–æ–≤
                len(user.selected_domains or []),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
                
                # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-–∏–¥–µ—è
                1 if idea.domain in (user.selected_domains or []) else 0,  # –∏–¥–µ—è –≤ –¥–æ–º–µ–Ω–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            ]
            
            training_data.append(features)
            labels.append(1 if swipe.swipe else 0)
        
        return np.array(training_data), np.array(labels)
    
    
    def train_content_based_model(self, ideas: List[Idea]):
        """–û–±—É—á–∞–µ—Ç content-based –º–æ–¥–µ–ª—å"""
        
        self.ideas_df = self._prepare_idea_features(ideas)
        
        if len(self.ideas_df) < 2:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–¥–µ–π –¥–ª—è content-based –º–æ–¥–µ–ª–∏")
            return
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–¥—Å—Ç–≤–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é
        tfidf_columns = [col for col in self.ideas_df.columns if col.startswith('tfidf_')]
        if tfidf_columns:
            tfidf_matrix = self.ideas_df[tfidf_columns].values
            self.content_similarity_matrix = cosine_similarity(tfidf_matrix)
        
        print(f"‚úÖ Content-based –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ {len(ideas)} –∏–¥–µ—è—Ö")
    
    
    def train_user_based_model(self, db_session, users: List[User]):
        """–û–±—É—á–∞–µ—Ç user-based –º–æ–¥–µ–ª—å"""
        
        self.users_df = self._prepare_user_features(db_session, users)
        
        if len(self.users_df) < 2:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è user-based –º–æ–¥–µ–ª–∏")
            return
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–¥—Å—Ç–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        feature_columns = ['like_ratio', 'avg_tags_per_like', 'liked_domains_ratio']
        available_columns = [col for col in feature_columns if col in self.users_df.columns]
        
        if available_columns:
            user_features = self.users_df[available_columns].fillna(0)
            user_features_scaled = self.scaler.fit_transform(user_features)
            self.user_similarity_matrix = cosine_similarity(user_features_scaled)
        
        print(f"‚úÖ User-based –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ {len(users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö")
    
    
    def train_ensemble_model(self, db_session):
        """–û–±—É—á–∞–µ—Ç ensemble –º–æ–¥–µ–ª—å"""
        
        X, y = self._prepare_training_data(db_session)
        
        if len(X) < 10:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ensemble –º–æ–¥–µ–ª–∏")
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ –≤—ã–±–æ—Ä–∫–µ –µ—Å—Ç—å –∫–∞–∫ –º–∏–Ω–∏–º—É–º –¥–≤–∞ –∫–ª–∞—Å—Å–∞ (–∏ –ª–∞–π–∫–∏, –∏ –¥–∏–∑–ª–∞–π–∫–∏)
        unique_classes = np.unique(y)
        if unique_classes.shape[0] < 2:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–Ω—É–∂–Ω—ã –∏ –ª–∞–π–∫–∏, –∏ –¥–∏–∑–ª–∞–π–∫–∏)")
            # –°–æ—Ö—Ä–∞–Ω–∏–º –ø–æ—è—Å–Ω–µ–Ω–∏–µ –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö, —á—Ç–æ–±—ã –æ—Ç–¥–∞—Ç—å –µ–≥–æ —á–µ—Ä–µ–∑ /api/ml/metrics
            self.training_metrics["ensemble"] = {
                "error": "single_class",
                "message": "Training requires at least two classes in swipe labels",
                "positive_rate": float(y.mean()) if len(y) > 0 else 0.0,
                "samples": int(len(y))
            }
            # –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–µ–º
            self.ensemble_model = None
            return
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # –û–±—É—á–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π
        models = {
            'logistic': LogisticRegression(random_state=42),
            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingClassifier(random_state=42)
        }
        
        best_model = None
        best_score = 0
        
        for name, model in models.items():
            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=3, scoring='accuracy')
            
            # –û–±—É—á–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, zero_division=0)
            recall = recall_score(y_test, y_pred, zero_division=0)
            f1 = f1_score(y_test, y_pred, zero_division=0)
            
            metrics = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            
            self.training_metrics[name] = metrics
            
            print(f"üìä {name}: Accuracy={accuracy:.3f}, F1={f1:.3f}")
            
            if accuracy > best_score:
                best_score = accuracy
                best_model = model
        
        self.ensemble_model = best_model
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model_path = os.path.join(self.model_dir, 'ensemble_model.joblib')
        joblib.dump(self.ensemble_model, model_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
        scaler_path = os.path.join(self.model_dir, 'scaler.joblib')
        joblib.dump(self.scaler, scaler_path)
        
        print(f"‚úÖ Ensemble –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞. –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_score:.3f}")
    
    
    def predict_user_preference(self, db_session, user: User, idea: Idea) -> Dict:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫ –∏–¥–µ–µ"""
        
        if not self.ensemble_model:
            return {"probability": 0.5, "confidence": "low", "method": "random"}
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            user_likes = get_user_likes(db_session, user.id)
            user_history = get_user_swipe_history(db_session, user.id)
            
            combined_text = f"{idea.title} {idea.description} {' '.join(idea.tags)}"
            
            features = [
                len(combined_text),
                len(idea.tags),
                self.domain_encoder.transform([idea.domain])[0] if hasattr(self.domain_encoder, 'classes_') and idea.domain in self.domain_encoder.classes_ else 0,
                len(user_history),
                len(user_likes),
                len(user_likes) / len(user_history) if user_history else 0,
                len(user.selected_domains or []),
                1 if idea.domain in (user.selected_domains or []) else 0,
            ]
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            features_scaled = self.scaler.transform([features])
            probability = self.ensemble_model.predict_proba(features_scaled)[0][1]
            
            # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = "high" if abs(probability - 0.5) > 0.3 else "medium" if abs(probability - 0.5) > 0.1 else "low"
            
            return {
                "probability": float(probability),
                "confidence": confidence,
                "method": "ensemble_ml"
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return {"probability": 0.5, "confidence": "low", "method": "fallback"}
    
    
    def get_recommendations(self, db_session, user: User, ideas: List[Idea], top_k: int = 10) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ø-K —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        
        recommendations = []
        
        for idea in ideas:
            prediction = self.predict_user_preference(db_session, user, idea)
            
            recommendations.append({
                "idea": idea,
                "probability": prediction["probability"],
                "confidence": prediction["confidence"],
                "method": prediction["method"]
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        recommendations.sort(key=lambda x: x["probability"], reverse=True)
        
        return recommendations[:top_k]
    
    
    def get_feature_importance(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        
        if not self.ensemble_model or not hasattr(self.ensemble_model, 'feature_importances_'):
            return {}
        
        feature_names = [
            'text_length', 'tag_count', 'domain', 'user_history_length',
            'user_likes_count', 'like_ratio', 'selected_domains_count', 'domain_match'
        ]
        
        importances = self.ensemble_model.feature_importances_
        
        return dict(zip(feature_names, importances.tolist()))
    
    
    def get_training_metrics(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        return self.training_metrics


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç–∞–Ω—Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—è
advanced_recommender = AdvancedRecommender() 